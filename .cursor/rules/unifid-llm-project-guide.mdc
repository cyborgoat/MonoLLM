---
description: 
globs: 
alwaysApply: false
---
UnifiedLLM

## Introduction
This is a framwork that handles different source of LLM providers with a simplifed entrance, that allows user to use a single function to query different LLM models, the detailed features are shown below:

## Project setup
- Python 3.13
- Library mangament with uv

## Availabel Models
- Google Gemini: Gemini 2.5 Series
- Open AI: Most recent LLM models (up to 3)
- Anthropic: Sonnet 4.0, opus 4.0
- Qwen: Qwen3 Sereis, QwQ Series
- DeepSeek: R1 & V3
- VolceEngine: Doubao & DeepSeek


## LLM Provider Documentations

- Google Gemini: https://ai.google.dev/gemini-api/docs/text-generation
- Anthropic: https://docs.anthropic.com/en/api/overview
- OpenAI: https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses
- Qwen(DashScope): https://help.aliyun.com/zh/model-studio/use-qwen-by-calling-api
- DeepSeek: https://api-docs.deepseek.com/
- Volcengine: https://www.volcengine.com/docs/82379/1494384


## Key features
- User can set proxy/socks5 for all LLM calls.
- Allow user to choose stream mode or not
- Indicator shows if the model is a reasoning model
- Allow user to show the thinking steps
- Allow user to set temperature and max output tokens(some providers call it token budget) if applicable.
- Allow user to integrate with MCP if the model is enabled this feature
- If the model provider supports OpenAI protocal, use this way as preferred way.
- Model & Proxy configurations should be maintained by a json file for easier furture update.
